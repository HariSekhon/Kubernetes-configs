#
#  Author: Hari Sekhon
#  Date: 2019-11-25 13:47:16 +0000 (Mon, 25 Nov 2019)
#
#  vim:ts=2:sts=2:sw=2:et
#
#  https://github.com/harisekhon/kubernetes-templates
#
#  License: see accompanying Hari Sekhon LICENSE file
#
#  If you're using my code you're welcome to connect with me on LinkedIn
#  and optionally send me feedback to help improve or steer this or other code I publish
#
#  https://www.linkedin.com/in/HariSekhon
#

# ============================================================================ #
#                              D e p l o y m e n t
# ============================================================================ #

# https://kubernetes.io/docs/concepts/workloads/controllers/deployment

# This is your main point of deploying apps, and has lots of tricks and best practices baked in
#
# Edit / Delete as necessary for your deployment (it's easier than remembering / finding / adding all this stuff)

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: NAME
  namespace: NAMESPACE
  labels:
    app: APP
  annotations:
    # see also k8s_pod_disruption_budget.yaml
    cluster-autoscaler.kubernetes.io/safe-to-evict: True  # set to False for apps that don't like disruption
spec:
  paused: False
  revisionHistoryLimit: 10  # how many ReplicaSets specs to retain for rollbacks
  strategy:
    type: RollingUpdate
    rollingUpdate:
      #maxUnavailable: 25%
      maxUnavailable: 1
      #maxSurge: 25%
      maxSurge: 1
  selector:
    matchLabels:
      app: APP
      role: master
      tier: backend
  replicas: 3
  template:
    metadata:
      labels:
        app: APP
        role: master
        tier: backend
    spec:
      nodeSelector:
        disktype: ssd
        kubernetes.io/arch: amd64
        kubernetes.io/os: linux
        node.kubernetes.io/instance-type: m3.medium    # 1.17+
        # for persistent disks limited to 2 zones combined with GKE regional clusters
        topology.kubernetes.io/zone: europe-west2-a  # 1.17+
        topology.kubernetes.io/region: europe-west2    # 1.17+
        #myLabel: myFastNodes
        # GKE specific - hard requirement - prefer affinity below as safer soft requirement to still allow scheduling if node pool becomes unavailable (eg. recreated by Terraform)
        #cloud.google.com/gke-nodepool: POOL_NAME
      #priorityClassName: high-priority  # for business critical workloads to get priority access to the stable node pool instead of preemptible node pool, will evict lower priority pods to preemptible node pool (default 0) if necessary. Requires: priorityclass.yaml
      schedulerName: default-scheduler
      # https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#affinity-and-anti-affinity
      # XXX: not recommended in clusters greater than several hundred nodes due to significant CPU overhead slowing down scheduling
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 1  # 1 to 100, higher is stronger
              preference:
                - matchExpressions:
                  - key: accelerator-type
                    operator: In
                    values:
                      - gpu
                      - tpu
              # XXX: prefer non-preemptible VMs unless unavailable
            - weight: 100
              preference:
                matchExpressions:
                  - key: cloud.google.com/gke-preemptible
                    operator: DoesNotExist
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                # AND's everything under this matchExpression
                - key: kubernetes.io/e2e-az-name
                  operator: In # NotIn, Exists, DoesNotExist
                  # OR's the values
                  values:
                  - europe-west-1b
                  - europe-west-1c
                # XXX: remove this if the pod is safe to preempt
                #
                # Node preemption:
                # - shuts down pods ungracefully
                # - ignores PodDisruptionBudget
                # - invalidate Stateful guarantees and can lead to data loss !!
                # https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#kubernetes_constraint_violations
                # https://cloud.google.com/kubernetes-engine/docs/how-to/preemptible-vms#best_practices
                - key: cloud.google.com/gke-preemptible
                  operator: DoesNotExist
        #podAffinity:
        podAntiAffinity:
          # XXX: spread app across AZs
          # XXX: use preferred if num_replicas > num_AZs
#          preferredDuringSchedulingIgnoredDuringExecution:
#            - weight: 100
#              podAffinityTerm:
#                topologyKey: topology.kubernetes.io/zone
#                labelSelector:
#                  matchExpressions:
#                    - key: app
#                      operator: In
#                      values:
#                        - APP
          requiredDuringSchedulingIgnoredDuringExecution:
              # XXX: spread app across AZs
            - topologyKey: topology.kubernetes.io/zone
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - APP
              # XXX: spread app across hosts (default prefers spread, whereas this requires it)
            - topologyKey: kubernetes.io/hostname
              labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - APP
      # XXX: allow scheduling on reserved by default via taint non-preemptible VMs
      #      tip: taint the non-preemptible nodes eg. kubectl taint nodes node1 "cloud.google.com/gke-preemptible"="false":PreferNoSchedule
      #           then add tolerance so only select deployments get priority scheduling on stable non-preemptible nodes
      # ignore taints
      #tolerations:
      #  - key: "myKey"
      #    operator: "Exists"  # if myKey exists ignore NoSchedule
      #    operator: "Equal"   # if myKey's value = "value" ignore NoSchedule
      #      value: "value"
      #    effect: "NoSchedule"
      tolerations:
        - key: cloud.google.com/gke-preemptible
          operator: Equal
          value: "false"
          effect: PreferNoSchedule
      minReadySeconds: 0  # secs healthy before serving
      progressDeadlineSeconds: 120
      terminationGracePeriodSeconds: 120  # how long between SIGTERM and SIGKILL (kill -9) - may want to set higher than default 30 secs eg. 1800 for Cassandra
      dnsPolicy: ClusterFirst # use coredns, Default uses node's dns resolution
      restartPolicy: Always
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File  # FallbackToLogsOnError to use container logs if message file is empty upon container error
      #serviceAccountName: NAME
      securityContext:
        # see pod-security-policy.yaml for wider enforcement
        runAsNonRoot: true
        #runAsUser: 0   # not recommended
        fsGroup: 1000   # group to own volumeMounts so they can be written to by the user-level process
      initContainers:
        - name: init-files
          image: alpine/git
          command: ['git', 'clone', 'https://github.com/harisekhon/devops-python-tools /data']
          volumeMounts:
            - name: app-vol
              mountPath: /data
        - name: init-mysql-service
          image: busybox
          command: ['sh', '-c', 'until nslookup mysql; do echo waiting for mysql service DNS to come up; sleep 1; done']
        - name: init-mysql-endpoint
          image: busybox
          command: ['sh', '-c', 'until nc -z mysql 3306; do echo waiting for mysql endpoint port to come up; sleep 1; done']
      containers:
      - name: master
        image: k8s.gcr.io/APP:latest
        imagePullPolicy: IfNotPresent  # set to Always if replacing a docker_image:tag or using :latest in development
        ports:
          - containerPort: 8080
        #securityContext:
        #  privileged: true
        env:
          - name: HEAP_SIZE
            value: 1G
          - name: CASSANDRA_SEEDS
            value: cassandra-0.cassandra.svc.cluster.local,cassandra-1.cassandra.svc.cluster.local,cassandra-2.cassandra.svc.cluster.local
          - name: POD_IP
            valueFrom:
              fieldRef:
                fieldPath: status.podIP  # K8S API path
          - name: MYSECRET
            valueFrom:
              secretKeyRef:
                name: my-secret
                key: my-secret
        #lifecycle:
        # postStart:
        #   exec:
        #     command: ["/bin/sh", "-c", "echo Hello from the postStart handler > /usr/share/message"]
        # preStop:
        #   exec:
        #     # recent nginx-ingress already has this, but still a decent example to gracefully finish connections on your own nginx via SIGQUIT not SIGTERM
        #     # also remember to increase terminationGracePeriodSeconds
        #     command: ["/bin/sh","-c","nginx -s quit; while killall -0 nginx; do sleep 1; done"]
        readinessProbe:
          # http, tcp or exec
          httpGet:
            path: /healthz
            port: 8080
          # only uncomment if you can't use http check, must delete http check otherwise will get this error:
          # invalid: spec.containers[0].livenessProbe.tcpSocket: Forbidden: may not specify more than 1 handler type
          #tcpSocket:
          #  port: 8080
          initialDelaySeconds: 0 # default
          successThreshold: 1    # default
          failureThreshold: 3    # default
          periodSeconds: 10      # default (interval)
          timeoutSeconds: 1      # default
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          #tcpSocket:
          #  port: 8080
          initialDelaySeconds: 0 # default
          successThreshold: 1    # default
          failureThreshold: 3    # default
          periodSeconds: 10      # default (interval)
          timeoutSeconds: 1      # default
        resources:
          limits:
            cpu: 1
            memory: 4Gi  # GiB
          requests:
            # millicores - 1/1000 of a CPU / vCPU
            # 100m = 1/10th of CPU core
            cpu: 100m
            memory: 100Mi # MiB
        volumes:
          - name: APP-vol
            # GKE will automatically create the underlying disk for us using default storage provider's dynamic provisioner
            claimName: APP-dynamic-pvc
            readOnly: true
        volumeMounts:
          - name: APP-vol
            mountPath: /mnt/APP-vol
        securityContext:
          capabilities:
            add: ["IPC_LOCK"]  # allow mlock to avoid paging ram to disk, for security or performance eg. NoSQL low latency datastores
        lifecycle:
          # execute this when sending TERM to container
          preStop:
            # http call or exec
            exec:
              command:
                - /bin/sh
                - -c
                - nodetool drain

      # GCP Cloud SQL Proxy sidecar - app can then connect to DB at localhost
      # https://cloud.google.com/sql/docs/postgres/connect-kubernetes-engine
      - name: cloud-sql-proxy
        image: gcr.io/cloudsql-docker/gce-proxy:1.17
        command:
          - /cloud_sql_proxy
          - "-ip_address_types=PRIVATE"
          - "-instances=<INSTANCE_CONNECTION_NAME>"
        securityContext:
          # The default Cloud SQL proxy image runs as the
          # "nonroot" user and group (uid: 65532) by default.
          runAsNonRoot: true
